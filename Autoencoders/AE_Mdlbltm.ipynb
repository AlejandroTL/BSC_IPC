{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of the data is:  (763, 12088)\nThe shape of the subgroups is:  (763, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Medulloblastoma Files\\Medulloblastoma_Cavalli_VAE_data.csv', sep=',', na_values=\".\")\n",
    "print(\"The shape of the data is: \", data.shape)\n",
    "data = data.rename(columns={'Unnamed: 0': 'Patient'})\n",
    "\n",
    "subgroups = pd.read_csv('Medulloblastoma Files\\GSE85218_subgroups.csv', sep=' ',header=None)\n",
    "print(\"The shape of the subgroups is: \", subgroups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of the data is:  (285, 12088)\nThe shape of the subgroups is:  (285, 2)\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv('Medulloblastoma Files\\Medulloblastoma_Northcott_VAE_data.csv', sep=',', na_values=\".\")\n",
    "print(\"The shape of the data is: \", data_test.shape)\n",
    "data_test = data_test.rename(columns={'Unnamed: 0': 'Patient'})\n",
    "\n",
    "subgroups_test = pd.read_csv('Medulloblastoma Files\\GSE37382_subgroups.csv', sep=' ',header=None)\n",
    "print(\"The shape of the subgroups is: \", subgroups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Patient'],axis=1)\n",
    "data_test = data_test.drop(['Patient'],axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data) #(x - mu / s) almost all values between -1,1\n",
    "\n",
    "scaler.fit(data_test)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "train_dataset = torch.tensor(data.values).float()\n",
    "\n",
    "data_test = pd.DataFrame(data_test)\n",
    "test_dataset = torch.tensor(data_test.values).float()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "source": [
    "### Define the Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=kwargs[\"embedding_dim\"]\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=kwargs[\"embedding_dim\"], out_features=kwargs[\"bottleneck\"]\n",
    "        )\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=kwargs[\"bottleneck\"], out_features=kwargs[\"embedding_dim\"]\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=kwargs[\"embedding_dim\"], out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_input_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.relu(code)\n",
    "        \n",
    "        activation = self.decoder_input_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.tanh(activation)\n",
    "\n",
    "        return reconstructed, code\n",
    "\n",
    "model = Autoencoder(input_shape=12087, embedding_dim=2048, bottleneck=256)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "source": [
    "### Train and test functions\n",
    "#### Doesn't needed, the values of the trained model are stored in a dictionary some chunks below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataset)/dataloader.batch_size)):\n",
    "        data = data # we want the data, not the label\n",
    "        data = data.view(data.size(0), -1) #flat the data\n",
    "        optimizer.zero_grad() # reset the gradients back to zero\n",
    "        reconstruction, _ = model(data)  # compute reconstructions\n",
    "        #print(\"RECONSTRUCTION :\", reconstruction)\n",
    "        loss = criterion(reconstruction, data) #calculate reconstruction loss\n",
    "        running_loss += loss.item() \n",
    "        loss.backward() # compute accumulated gradients\n",
    "        optimizer.step() #update the weights (net.parameters)\n",
    "    train_loss = running_loss/len(dataloader.dataset) # average loss\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad(): # in validation we don't want to update weights\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(test_dataset)/dataloader.batch_size)):\n",
    "            data = data\n",
    "            data = data.view(data.size(0), -1)\n",
    "            reconstruction, _ = model(data)\n",
    "            loss = criterion(reconstruction, data)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "source": [
    "### Train the Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20   #the loss stuck up at this epoch\n",
    "batch_size = 8\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    test_epoch_loss = validate(model, test_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)\n",
    "    print(f\"\\nTrain Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "source": [
    "### Save model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './AE_MDLTBM.pth'\n",
    "torch.save(model.state_dict(), PATH) #save in a dictionary all parameters"
   ]
  },
  {
   "source": [
    "### Load model\n",
    "#### This chunk loads a trained AE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "PATH = './AE_MDLTBM.pth'\n",
    "model = Autoencoder(input_shape=12087, embedding_dim=2048, bottleneck=256)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "source": [
    "### Function to get the embeddings and the reconstrunctions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model,dataloader):\n",
    "    model.eval()\n",
    "    rec_model = np.zeros(shape=(0,12087))\n",
    "    embedding_model = np.zeros(shape=(0,256))\n",
    "    with torch.no_grad(): # in validation we don't want to update weights\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(test_dataset)/dataloader.batch_size)):\n",
    "            data = data.view(data.size(0), -1)\n",
    "            reconstruction, coded = model(data)\n",
    "            rec_model = np.concatenate((rec_model, reconstruction), axis=0)\n",
    "            embedding_model = np.concatenate((embedding_model, coded), axis=0)\n",
    "    return rec_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "36it [00:02, 13.99it/s]\n"
     ]
    }
   ],
   "source": [
    "reconstructed, coded = get_embeddings(model, test_loader)"
   ]
  },
  {
   "source": [
    "### UMAP to visualize the data. I visualize the embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.patches as mpatches\n",
    "import umap\n",
    "\n",
    "%matplotlib qt\n",
    "n_neighbors = [5,15,50,75,100,200]\n",
    "\n",
    "for i in n_neighbors:\n",
    "    reducer = umap.UMAP(n_components=3,n_neighbors=i)\n",
    "    embedding = reducer.fit_transform(data_test)\n",
    "    embedding_df = pd.DataFrame(embedding)\n",
    "    embedding_df['Subgroups']= subgroups_test[1].values\n",
    "\n",
    "    X_data= embedding_df[0]\n",
    "    Y_data = embedding_df[1]\n",
    "    Z_data = embedding_df[2]\n",
    "    Sbgrp = embedding_df['Subgroups']\n",
    "\n",
    "    cdict = {'Group4': 'red', 'SHH': 'blue', 'WNT': 'green', 'Group3': 'yellow'}\n",
    "    c = [cdict[val] for val in Sbgrp]\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(X_data, Y_data, Z_data, c=c);\n",
    "    red_c = mpatches.Patch(color='red', label='Group4')\n",
    "    blue_c = mpatches.Patch(color='blue', label='SHH')\n",
    "    green_c = mpatches.Patch(color='green', label='WNT')\n",
    "    yellow_c = mpatches.Patch(color='yellow', label='Group3')\n",
    "    plt.legend(handles=[red_c,blue_c,green_c,yellow_c])\n",
    "    plt.title('UMAP with n_neighbors %i'%(i))\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "### Now we get, per every observation, with which observation has the greater cosine similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_cosine(coded):\n",
    "    indexes = []\n",
    "    for i in range(0,len(coded)):\n",
    "        minor = -1\n",
    "        index = 0\n",
    "        for j in range(0,len(coded)):\n",
    "            if i != j:\n",
    "                similarity = cosine_similarity(coded.iloc[i].values.reshape(1,256),coded.iloc[j].values.reshape(1,256))\n",
    "                if similarity > minor:\n",
    "                    minor = similarity\n",
    "                    index = j\n",
    "        if index < np.argmax(subgroups_test[1]=='Group3'):\n",
    "            indexes.append('SHH')\n",
    "        elif np.argmax(subgroups_test[1]=='Group3') < index < np.argmax(subgroups_test[1]=='Group4'):\n",
    "            indexes.append('Group3')\n",
    "        else:\n",
    "            indexes.append('Group4')\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "coded = pd.DataFrame(coded)\n",
    "indexes = comparison_cosine(coded)\n",
    "subgroups_test['Match'] = indexes"
   ]
  },
  {
   "source": [
    "### Which percentage of observations has the greater cosine similarity with a observation of its own subgroup?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The % of observations that have the greatest cosine similarity with a observation of its own subgroups is:  93.33333\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "mismatch = []\n",
    "for i in range(0,len(subgroups_test)):\n",
    "    mismatch_list = []\n",
    "    if subgroups_test['Match'].iloc[i] == subgroups_test[1].iloc[i]:\n",
    "        match = match+1\n",
    "    else:\n",
    "       mismatch_list.append(subgroups_test[1].iloc[i])\n",
    "       mismatch_list.append(subgroups_test['Match'].iloc[i])\n",
    "       mismatch_list.append(i)\n",
    "       mismatch.append(mismatch_list)\n",
    "total = match/len(subgroups_test)\n",
    "print(\"The % of observations that have the greatest cosine similarity with a observation of its own subgroups is: \", round((total*100),5))\n"
   ]
  },
  {
   "source": [
    "### The observations that doesn't match with a observation of its subgroup, with which subgroup match?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['SHH', 'Group4', 6],\n",
       " ['SHH', 'Group4', 35],\n",
       " ['Group3', 'Group4', 54],\n",
       " ['Group3', 'Group4', 58],\n",
       " ['Group3', 'Group4', 71],\n",
       " ['Group3', 'SHH', 89],\n",
       " ['Group3', 'Group4', 91],\n",
       " ['Group3', 'Group4', 95],\n",
       " ['Group3', 'Group4', 96],\n",
       " ['Group4', 'Group3', 106],\n",
       " ['Group4', 'Group3', 111],\n",
       " ['Group4', 'Group3', 134],\n",
       " ['Group4', 'Group3', 172],\n",
       " ['Group4', 'Group3', 195],\n",
       " ['Group4', 'Group3', 241],\n",
       " ['Group4', 'SHH', 244],\n",
       " ['Group4', 'Group3', 256],\n",
       " ['Group4', 'Group3', 273],\n",
       " ['Group4', 'Group3', 278]]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "mismatch"
   ]
  },
  {
   "source": [
    "### It can be seen that the greatest number of mismatch happens between G3 and G4\n",
    "#### Something interesting is that it can be seen that some SHH and G4 are matchs, and this can be also seen in the UMAP. There are some red points near of blue points.\n",
    "\n",
    "### Now let's compute a standard of each subgroup with the average values and calculate the cosine similarity between these standards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cosine similarity SHH-G3:  [[0.79669056]]\nCosine similarity SHH-G4:  [[0.78173266]]\nCosine similarity G4-G3:  [[0.876827]]\n"
     ]
    }
   ],
   "source": [
    "coded['Subgroup'] = subgroups_test[1] #add subgroups column to the embedding data\n",
    "\n",
    "mean_shh = coded[coded['Subgroup'] == 'SHH']\n",
    "mean_g3 = coded[coded['Subgroup'] == 'Group3']\n",
    "mean_g4 = coded[coded['Subgroup'] == 'Group4']\n",
    "\n",
    "standard_shh = mean_shh.mean().values.reshape(1,256)\n",
    "standard_g3 = mean_g3.mean().values.reshape(1,256)\n",
    "standard_g4 = mean_g4.mean().values.reshape(1,256)\n",
    "\n",
    "coded = coded.drop(['Subgroup'],axis=1)\n",
    "\n",
    "cosine_shh_g3 = cosine_similarity(standard_shh,standard_g3)\n",
    "cosine_shh_g4 = cosine_similarity(standard_shh,standard_g4)\n",
    "cosine_g4_g3 = cosine_similarity(standard_g4,standard_g3)\n",
    "print(\"Cosine similarity SHH-G3: \", cosine_shh_g3)\n",
    "print(\"Cosine similarity SHH-G4: \", cosine_shh_g4)\n",
    "print(\"Cosine similarity G4-G3: \", cosine_g4_g3)"
   ]
  },
  {
   "source": [
    "### The greatest cosine similarity is between G3 and G4\n",
    "\n",
    "### Now it's computed the cosine similarity between observations and the standards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_cosine_standard(coded):\n",
    "    indexes = []\n",
    "    for i in range(0,len(coded)):\n",
    "        minor = -1\n",
    "        cosine_shh = cosine_similarity(coded.iloc[i].values.reshape(1,256),standard_shh)\n",
    "        cosine_g3 = cosine_similarity(coded.iloc[i].values.reshape(1,256),standard_g3)\n",
    "        cosine_g4 = cosine_similarity(coded.iloc[i].values.reshape(1,256),standard_g4)\n",
    "        if max(cosine_shh,cosine_g3,cosine_g4) == cosine_shh:\n",
    "            indexes.append('SHH')\n",
    "        elif max(cosine_shh,cosine_g3,cosine_g4) == cosine_g3:\n",
    "            indexes.append('Group3')\n",
    "        else: \n",
    "            indexes.append('Group4')\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_stardard = comparison_cosine_standard(coded)\n",
    "subgroups_test['Match_standard'] = indexes_stardard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The % of observations that have the greatest cosine similarity with the standard of its own subgroups is:  97.89474\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "mismatch = []\n",
    "for i in range(0,len(subgroups_test)):\n",
    "    mismatch_list = []\n",
    "    if subgroups_test['Match_standard'].iloc[i] == subgroups_test[1].iloc[i]:\n",
    "        match = match+1\n",
    "    else:\n",
    "        mismatch_list.append(subgroups_test[1].iloc[i])\n",
    "        mismatch_list.append(subgroups_test['Match_standard'].iloc[i])\n",
    "        mismatch_list.append(i)\n",
    "        mismatch.append(mismatch_list)\n",
    "total = match/len(subgroups_test)\n",
    "print(\"The % of observations that have the greatest cosine similarity with the standard of its own subgroups is: \", round((total*100),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Group3', 'Group4', 71],\n",
       " ['Group3', 'Group4', 91],\n",
       " ['Group4', 'Group3', 106],\n",
       " ['Group4', 'SHH', 244],\n",
       " ['Group4', 'SHH', 256],\n",
       " ['Group4', 'Group3', 278]]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}