{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bscuser\\anaconda3\\envs\\pytorch_medulloblastoma\\lib\\site-packages\\umap\\__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.patches as mpatches\n",
    "import umap\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_generated(data,colors):\n",
    "    \n",
    "    if type(data) == torch.tensor:\n",
    "        data = data.numpy()\n",
    "    \n",
    "    minor_list = []\n",
    "    index_list = []\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        minor = 9999\n",
    "        index = 0\n",
    "        for j in range(0,len(data)):\n",
    "            if i != j:\n",
    "                a=LA.norm(data[i]-data[j],ord=1)\n",
    "                if colors[j] in ['Generated','Reference1','Reference2']: \n",
    "                    a = 9999\n",
    "                if a < minor:\n",
    "                    minor = a\n",
    "                    index = j\n",
    "        print(i, minor, index, colors[i], colors[index])\n",
    "        minor_list.append(minor)\n",
    "        if colors[i] == 'Generated':\n",
    "            index_list.append(index)\n",
    "  #  print(\"Average nearest distance w/o Generated:\", np.cumsum(minor_list[:285])[-1]/len(minor_list[:285]))\n",
    "   # print(\"Average nearest distance Generated:\", np.cumsum(minor_list[285:])[-1]/len(minor_list[285:]))\n",
    "\n",
    "    return minor_list, set(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metapatients(data, subgroups):\n",
    "    mean_df = pd.DataFrame(data) #mean = coded\n",
    "    mean_df['Subgroup'] = subgroups[1] #add subgroups column to the embedding data\n",
    "\n",
    "    mean_shh = mean_df[mean_df['Subgroup'] == 'SHH']\n",
    "    mean_g3 = mean_df[mean_df['Subgroup'] == 'Group3']\n",
    "    mean_g4 = mean_df[mean_df['Subgroup'] == 'Group4']\n",
    "    #mean_wnt = mean_df[mean_df['Subgroup'] == 'WNT']\n",
    "\n",
    "    # Centroids of Means\n",
    "    standard_shh = mean_shh.mean().values.reshape(1,len(data[0])) \n",
    "    standard_g3 = mean_g3.mean().values.reshape(1,len(data[0]))\n",
    "    standard_g4 = mean_g4.mean().values.reshape(1,len(data[0]))\n",
    "    #standard_wnt = mean_wnt.mean().values.reshape(1,len(data[0]))\n",
    "\n",
    "    return standard_g3, standard_g4, standard_shh#, standard_wnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_plot(data,color,components=2):\n",
    "    \n",
    "    #n_neighbors = [5,15,22,30]\n",
    "    n_neighbors = [15]\n",
    "    \n",
    "\n",
    "    for i in n_neighbors:\n",
    "        reducer = umap.UMAP(n_components=components,n_neighbors=i, densmap=True)\n",
    "        embedding = reducer.fit_transform(data)\n",
    "        embedding_df = pd.DataFrame(embedding)\n",
    "        embedding_df['Subgroups']= color\n",
    "\n",
    "        X_data= embedding_df[0]\n",
    "        Y_data = embedding_df[1]\n",
    "        if components == 3:\n",
    "            Z_data = embedding_df[2]\n",
    "        Sbgrp = embedding_df['Subgroups']\n",
    "\n",
    "        cdict = {'Group4': 'pink', 'SHH': 'blue', 'WNT': 'green', 'Group3': 'gold', 'Generated': 'black', 'Reference1': 'red', 'Reference2': 'red'}\n",
    "        c = [cdict[val] for val in Sbgrp]\n",
    "\n",
    "        plt.figure(figsize=(16,10))\n",
    "        if components == 3:\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter3D(X_data, Y_data, Z_data, c=c)\n",
    "        if components == 2:\n",
    "            plt.scatter(X_data,Y_data,c=c)\n",
    "        pink_c = mpatches.Patch(color='pink', label='Group4')\n",
    "        blue_c = mpatches.Patch(color='blue', label='SHH')\n",
    "        green_c = mpatches.Patch(color='green', label='WNT')\n",
    "        yellow_c = mpatches.Patch(color='gold', label='Group3')\n",
    "        black_c = mpatches.Patch(color='black', label='Generated')\n",
    "        red_c = mpatches.Patch(color='red', label='Reference')\n",
    "        plt.legend(handles=[pink_c,blue_c,green_c,yellow_c,black_c,red_c])\n",
    "        plt.title('UMAP with n_neighbors %i'%(i))\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model,dataloader):\n",
    "    model.eval()\n",
    "    rec_model = np.zeros(shape=(0,model.decoder[2].out_features))\n",
    "    embedding_model = np.zeros(shape=(0,features))\n",
    "    mean_model = np.zeros(shape=(0,features))\n",
    "    logvar_model = np.zeros(shape=(0,features))\n",
    "    with torch.no_grad(): # in validation we don't want to update weights\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(test_dataset)/dataloader.batch_size)):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            reconstruction,mean,logvar, coded = model(data)\n",
    "            rec_model = np.concatenate((rec_model, reconstruction), axis=0)\n",
    "            mean_model = np.concatenate((mean_model, mean), axis=0)\n",
    "            logvar_model = np.concatenate((logvar_model, logvar), axis=0)\n",
    "            embedding_model = np.concatenate((embedding_model,coded),axis=0)\n",
    "    return rec_model, embedding_model, mean_model, logvar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(N,subgroup,test_dataset):\n",
    "\n",
    "  #  if subgroup == 'G4':\n",
    "  #      data = meta_g4\n",
    "  #  elif subgroup == 'SHH':\n",
    "  #      data = meta_shh\n",
    "  #  elif subgroup == 'G3':\n",
    "  #      data = meta_g3\n",
    "  #  else:\n",
    "  #      print(\"Incorrect subgroup\")\n",
    "  #      return    \n",
    "\n",
    "    data = test_dataset[subgroup]\n",
    "    \n",
    "    sample = np.zeros(shape=(1,features))\n",
    "\n",
    "  #  data = torch.from_numpy(data).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        reconstruction, mean, logvar, coded = model(data)          \n",
    "\n",
    "    for i in range(0,N):\n",
    "        std = torch.exp(0.5*logvar) \n",
    "        eps = torch.randn_like(std) \n",
    "        resultado = mean + (eps*std)\n",
    "        sample = np.concatenate((sample, resultado), axis=0)\n",
    "\n",
    "    sample = sample.reshape(N+1,features)\n",
    "    z = sample[1:]\n",
    "\n",
    "    z = torch.from_numpy(z)\n",
    "    z = z.float()\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        z = z.to(device)\n",
    "        samples = model.decoder(z)   #decode the data\n",
    "    generated = torch.cat([test_dataset, samples], dim=0) #concat the test data and the generate data to visualize it\n",
    "    new_colors = np.array(['Generated']*len(samples)) #create the reference to paint black the generate examples\n",
    "    colors_generated = np.concatenate((colors,new_colors),axis=0) #concat the colors of the test data and the generate data\n",
    "\n",
    "    return generated, colors_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_interpolation(N,patient1,patient2, colors, test_dataset,logvar_true = True):\n",
    "\n",
    "    z1 = torch.from_numpy(patient1).float() #Means\n",
    "    z2 = torch.from_numpy(patient2).float() #Logvar\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        z1 = z1.to(device)\n",
    "        z2 = z2.to(device)\n",
    "        reference1, mean1, logvar1, _ = model(z1)                \n",
    "        reference2, mean2, logvar2, _ = model(z2)  \n",
    "\n",
    "    sample = np.zeros(shape=(1,features))\n",
    "    for i in range(0,N):\n",
    "        mean = i / (N - 1) * mean2 + (1 - i / (N - 1) ) * mean1 #interpolation mean\n",
    "        if logvar_true == True:\n",
    "            std1 = torch.exp(0.5*logvar1)\n",
    "            std2 = torch.exp(0.5*logvar2)\n",
    "            std = i / (N - 1) * std2 + (1 - i / (N - 1) ) * std1 #interpolation logvar\n",
    "        else:\n",
    "            std = 0\n",
    "        eps = torch.randn_like(std) \n",
    "        resultado = mean + (eps*std)\n",
    "        sample = np.concatenate((sample, resultado), axis=0)\n",
    "    sample = sample.reshape(N+1,features)\n",
    "    z = sample[1:]\n",
    "\n",
    "    z = torch.from_numpy(z) #preprocessing to introduce samples in NN\n",
    "    z = z.float()\n",
    "\n",
    "    \n",
    "    #GENERATE INTERPOLATION DATA\n",
    "    with torch.no_grad():        \n",
    "        z = z.to(device)\n",
    "        samples = model.decoder(z)   #decode the data\n",
    "    generated = torch.cat([test_dataset, samples], dim=0) #concat the test data and the generate data to visualize it\n",
    "    new_colors = np.array(['Generated']*len(samples)) #create the reference to paint black the generate examples\n",
    "    colors_generated = np.concatenate((colors,new_colors),axis=0) #concat the colors of the test data and the generate data\n",
    "\n",
    "    # ADD REFERENCES\n",
    "    generated = torch.cat((generated,reference1),axis=0) # add the centroids to the data to plot them\n",
    "    colors_reference = np.array(['Reference1']) #add label generated\n",
    "    colors_generated = np.concatenate((colors_generated,colors_reference),axis=0) #generate new colors\n",
    "\n",
    "    generated = torch.cat((generated,reference2),axis=0) # add the centroids to the data to plot them\n",
    "    colors_reference = np.array(['Reference2']) #add label generated\n",
    "    colors_generated = np.concatenate((colors_generated,colors_reference),axis=0) #generate new colors\n",
    "\n",
    "    return generated, colors_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_interpolation_generation(N,patient1,patient2, colors, test_dataset,logvar_true = True):\n",
    "\n",
    "    z1 = torch.from_numpy(patient1).float() #Means\n",
    "    z2 = torch.from_numpy(patient2).float() #Logvar\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z1 = z1.to(device)\n",
    "        z2 = z2.to(device)\n",
    "        reference1, mean1, logvar1, _ = model(z1)                \n",
    "        reference2, mean2, logvar2, _ = model(z2)  \n",
    "\n",
    "    sample = np.zeros(shape=(1,features))\n",
    "    for i in range(0,N):\n",
    "        mean = i / (N - 1) * mean2 + (1 - i / (N - 1) ) * mean1 #interpolation mean\n",
    "        if logvar_true == True:\n",
    "            std1 = torch.exp(0.5*logvar1)\n",
    "            std2 = torch.exp(0.5*logvar2)\n",
    "            std = i / (N - 1) * std2 + (1 - i / (N - 1) ) * std1 #interpolation logvar\n",
    "        else:\n",
    "            std = 0\n",
    "        eps = torch.randn_like(std) \n",
    "        resultado = mean + (eps*std)\n",
    "        sample = np.concatenate((sample, resultado), axis=0)\n",
    "    sample = sample.reshape(N+1,features)\n",
    "    z = sample[1:]\n",
    "\n",
    "    z = torch.from_numpy(z) #preprocessing to introduce samples in NN\n",
    "    z = z.float()\n",
    "\n",
    "    \n",
    "    #GENERATE INTERPOLATION DATA\n",
    "    with torch.no_grad():      \n",
    "        z = z.to(device)\n",
    "        samples = model.decoder(z)   #decode the data\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is:  (763, 5669)\n",
      "The shape of the subgroups is:  (763, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Medulloblastoma Files\\Medulloblastoma_Cavalli_VAE_data_Less.csv', sep=',', na_values=\".\")\n",
    "print(\"The shape of the data is: \", data.shape)\n",
    "data = data.rename(columns={'Unnamed: 0': 'Patient'})\n",
    "\n",
    "subgroups = pd.read_csv('Medulloblastoma Files\\GSE85218_subgroups.csv', sep=' ',header=None)\n",
    "print(\"The shape of the subgroups is: \", subgroups.shape)\n",
    "colors_train = subgroups[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is:  (285, 5669)\n",
      "The shape of the subgroups is:  (285, 2)\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv('Medulloblastoma Files\\Medulloblastoma_Northcott_VAE_data_Less.csv', sep=',', na_values=\".\")\n",
    "print(\"The shape of the data is: \", data_test.shape)\n",
    "data_test = data_test.rename(columns={'Unnamed: 0': 'Patient'})\n",
    "\n",
    "subgroups_test = pd.read_csv('Medulloblastoma Files\\GSE37382_subgroups.csv', sep=' ',header=None)\n",
    "print(\"The shape of the subgroups is: \", subgroups_test.shape)\n",
    "colors = subgroups_test[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Patient'],axis=1)\n",
    "data_test = data_test.drop(['Patient'],axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data) #(x - mu / s) almost all values between -1,1\n",
    "\n",
    "scaler.fit(data_test)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-bb16b4359ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Patient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Patient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "data = data.drop(['Patient'],axis=1)\n",
    "data_test = data_test.drop(['Patient'],axis=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data) #(x - mu / s) almost all values between -1,1\n",
    "\n",
    "scaler.fit(data_test)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_plot(data_test,colors,components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "train_dataset = torch.tensor(data.values).float()\n",
    "\n",
    "data_test = pd.DataFrame(data_test)\n",
    "test_dataset = torch.tensor(data_test.values).float()\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bscuser\\anaconda3\\envs\\pytorch_medulloblastoma\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "features = 32\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=kwargs[\"input_shape\"], out_features=kwargs[\"mid_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=kwargs[\"mid_dim\"], out_features=features*2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=features, out_features=kwargs[\"mid_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=kwargs[\"mid_dim\"], out_features=kwargs[\"input_shape\"]),\n",
    "            nn.Tanh()\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparametrize(self, mu, log_var):\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*log_var) \n",
    "            eps = torch.randn_like(std) \n",
    "            sample = mu + (eps*std) \n",
    "        else:\n",
    "            sample = mu\n",
    "        return sample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu_logvar = self.encoder(x).view(-1,2,features)\n",
    "        mu = mu_logvar[:, 0, :] \n",
    "        log_var = mu_logvar[:, 1, :] \n",
    "\n",
    "        z = self.reparametrize(mu,log_var) \n",
    "        reconstruction = self.decoder(z)\n",
    "        \n",
    "        return reconstruction, mu, log_var, z\n",
    "    \n",
    "\n",
    "#model = VAE(input_shape=5668, mid_dim=2048)\n",
    "#criterion = nn.BCELoss(reduction='sum')\n",
    "#criterion = nn.MSELoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mu, logvar, reconstruction_loss, beta):\n",
    "    HALF_LOG_TWO_PI = 0.91893\n",
    "    beta = 0.000001\n",
    "    KL_divergence = beta * 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) \n",
    "    gamma = torch.sqrt(reconstruction_loss)\n",
    "    log_gamma = torch.log(gamma)\n",
    "    Reconstruction = reconstruction_loss/(2*gamma) + log_gamma + HALF_LOG_TWO_PI\n",
    "\n",
    "    return KL_divergence + Reconstruction#, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, beta):\n",
    "    model.train() #network in train mode\n",
    "    running_loss = 0.0\n",
    "    gamma_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataset)/dataloader.batch_size)):\n",
    "        data = data # we want the data, not the label\n",
    "        data = data.view(data.size(0), -1) #flat the data\n",
    "        optimizer.zero_grad() # reset the gradients back to zero\n",
    "        reconstruction, mu, logvar,_ = model(data)  # compute reconstructions\n",
    "        reconstruction_loss = criterion(reconstruction, data) #calculate reconstruction loss\n",
    "        loss = final_loss(mu, logvar, reconstruction_loss, beta)# real loss: reconstruction + kl_divergence\n",
    "        running_loss += loss.item() \n",
    "        loss.backward() # compute accumulated gradients\n",
    "        optimizer.step() #update the weights (net.parameters)\n",
    "    train_loss = running_loss/len(dataloader.dataset) # average loss\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, beta):\n",
    "    model.eval() #network in evaluation mode\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad(): # in validation we don't want to update weights\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(test_dataset)/dataloader.batch_size)):\n",
    "            data = data\n",
    "            data = data.view(data.size(0), -1)\n",
    "            reconstruction, mu, logvar, coded = model(data)\n",
    "            reconstruction_loss = criterion(reconstruction, data)\n",
    "            loss  = final_loss(mu, logvar, reconstruction_loss, beta)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 200  #the loss stuck up at this epoch\n",
    "batch_size = 16\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n Epoch {epoch+1} of {epochs}\")\n",
    "    if epoch < 10:\n",
    "        beta_launcher = 0\n",
    "    elif 20 <= epoch <= 40:\n",
    "        beta_launcher = (0.05*epoch - 1)\n",
    "    elif epoch > 40:\n",
    "        beta_launcher = 1\n",
    "    test_epoch_loss = validate(model, test_loader, beta_launcher)\n",
    "    train_epoch_loss = fit(model, train_loader, beta_launcher)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)\n",
    "    print(f\"\\nTrain Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")\n",
    "    print(f\"Betha value: {beta_launcher: 4f}\")\n",
    "plot_train_test(train_loss,test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './vaeAnnealing32MSE.pth'\n",
    "torch.save(model.state_dict(), PATH) #save in a dictionary all paramete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './vaeAnnealing32MSE.pth'\n",
    "model = VAE(input_shape=5668, mid_dim=2048).to(device)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed, coded, mean, logvar = get_embeddings(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_g3, meta_g4, meta_shh = metapatients(reconstructed, subgroups_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_distance_g3_shh = LA.norm(meta_shh - meta_g3,ord=1)\n",
    "M_distance_G3G4 = LA.norm(meta_g4 - meta_g3,ord=1)\n",
    "M_distance_shh_G4 = LA.norm(meta_g4 - meta_shh,ord=1)\n",
    "\n",
    "print(\"Manhattan Distance G3-SHH: \",M_distance_g3_shh)\n",
    "print(\"Manhattan Distance G3-G4: \",M_distance_G3G4)\n",
    "print(\"Manhattan Distance SHH-G4: \",M_distance_shh_G4)\n",
    "\n",
    "E_distance_g3_shh = LA.norm(meta_shh - meta_g3,ord=2)\n",
    "E_distance_G3G4 = LA.norm(meta_g4 - meta_g3,ord=2)\n",
    "E_distance_shh_G4 = LA.norm(meta_g4 - meta_shh,ord=2)\n",
    "\n",
    "print(\"\\nEuclidean Distance G3-SHH: \",E_distance_g3_shh)\n",
    "print(\"Euclidean Distance G3-G4: \",E_distance_G3G4)\n",
    "print(\"Euclidean Distance SHH-G4: \",E_distance_shh_G4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = umap_plot(mean,colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, new_colors = data_generation(30, 80, test_dataset) #generate 64 data of G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_plot(generated,new_colors) #plot it in UMAP with 2 dimensions\n",
    "#umap_plot(generated,new_colors,3) plot it in UMAP with 3 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_data, interpolation_colors = data_interpolation(32,test_dataset[95].detach().numpy(),test_dataset[200].detach().numpy(),colors,test_dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_plot(interpolation_data,interpolation_colors,3) #plot it in UMAP with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_data, interpolation_colors = data_interpolation(32,meta_g3,meta_g4,colors,test_dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor, near_generated = distances_generated(interpolation_data,interpolation_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "\n",
    "for i in range(0,len(test_dataset)):\n",
    "    for j in range(0,len(test_dataset)):\n",
    "        if colors[i] != colors[j]:\n",
    "            interpolation_data, interpolation_colors = data_interpolation(32,test_dataset[i].detach().numpy(),test_dataset[j].detach().numpy(),colors,test_dataset, True)\n",
    "            minor, near_generated = distances_generated(interpolation_data,interpolation_colors)\n",
    "            if len(near_generated) >= 10:\n",
    "                print(\"Candidates: \", i, \" --- \",j)\n",
    "            else:\n",
    "                print(\"Nothing in \", i, \" --- \", j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_data, interpolation_colors = data_interpolation(32,test_dataset[2].detach().numpy(),test_dataset[175].detach().numpy(),colors,test_dataset, True)\n",
    "minor, near_generated = distances_generated(interpolation_data,interpolation_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.zeros(shape=(features,))\n",
    "\n",
    "N = 200\n",
    "z = torch.randn((N, features))\n",
    "with torch.no_grad():\n",
    "    sample = model.decoder(z)\n",
    "\n",
    "generated = torch.cat([train_dataset, sample], dim=0) #concat the test data and the generate data to visualize it\n",
    "new_colors = np.array(['Generated']*len(sample)) #create the reference to paint black the generate examples\n",
    "colors_generated = np.concatenate((colors_train,new_colors),axis=0) #concat the colors of the test data and the generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_plot(generated,colors_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = torch.cat([test_dataset, torch.from_numpy(meta_g3).float(), torch.from_numpy(meta_g4).float(), torch.from_numpy(meta_shh).float()], dim=0) #concat the test data and the generate data to visualize it\n",
    "new_colors = np.array(['Generated']*len(meta_g3)*3) #create the reference to paint black the generate examples\n",
    "colors_generated = np.concatenate((colors,new_colors),axis=0) #concat the colors of the test data and the generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
